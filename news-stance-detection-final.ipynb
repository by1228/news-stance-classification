{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Brian\n",
      "[nltk_data]     Yeung\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "from collections import Counter\n",
    "from stemming.porter2 import stem\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "bodies = pd.read_csv('fnc-1/train_bodies.csv')\n",
    "stances = pd.read_csv('fnc-1/train_stances.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/validation split: 90/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = random.Random()\n",
    "r.seed(12345)\n",
    "body_ids = bodies['Body ID'].tolist()\n",
    "r.shuffle(body_ids)\n",
    "train_ids = body_ids[:int(len(body_ids)*0.9)]\n",
    "validation_ids = body_ids[int(len(body_ids)*0.9):]\n",
    "\n",
    "train_bodies = bodies[bodies['Body ID'].isin(train_ids)].reset_index(drop=True)\n",
    "train_stances = stances[stances['Body ID'].isin(train_ids)].reset_index(drop=True)\n",
    "validation_bodies = bodies[bodies['Body ID'].isin(validation_ids)].reset_index(drop=True)\n",
    "validation_stances = stances[stances['Body ID'].isin(validation_ids)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stance\n",
       "agree         7.294118\n",
       "disagree      1.667037\n",
       "discuss      18.182020\n",
       "unrelated    72.856826\n",
       "Name: Body ID, dtype: float64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary statistics of stance distribution in training set\n",
    "train_stances.groupby('Stance')['Body ID'].count()/len(train_stances)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stance\n",
       "agree         7.964242\n",
       "disagree      1.808208\n",
       "discuss      14.587566\n",
       "unrelated    75.639984\n",
       "Name: Body ID, dtype: float64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary statistics of stance distribution in validation set\n",
    "validation_stances.groupby('Stance')['Body ID'].count()/len(validation_stances)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to convert a document into lower case and remove all symbols\n",
    "def clean_doc(string):\n",
    "    cleaned_string = string.lower().replace(',', ' ').replace('.', ' ').replace(';', ' ').replace(':', ' ').replace('(', '').replace(')', '').replace('[','').replace(']','').replace('\\'','').replace('\\\"','').replace('‘','').replace('’','').replace('“','').replace('”','').replace('/','').replace('?','').replace('!','').replace('%','').replace('&','').replace('-','').replace('$','').replace('—','')\n",
    "    return cleaned_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract unique words for all Headline and articleBody\n",
    "train_stances['Headline words'] = train_stances['Headline'].apply(lambda x: set(clean_doc(x).split()))\n",
    "train_bodies['articleBody words'] = train_bodies['articleBody'].apply(lambda x: set(clean_doc(x).split()))\n",
    "validation_stances['Headline words'] = validation_stances['Headline'].apply(lambda x: set(clean_doc(x).split()))\n",
    "validation_bodies['articleBody words'] = validation_bodies['articleBody'].apply(lambda x: set(clean_doc(x).split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_stances_stemmed = train_stances.copy()\n",
    "train_bodies_stemmed = train_bodies.copy()\n",
    "validation_stances_stemmed = validation_stances.copy()\n",
    "validation_bodies_stemmed = validation_bodies.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract stemmed words\n",
    "stem_words_train_bodies = []\n",
    "stem_words_train_headline = []\n",
    "stem_words_validation_bodies = []\n",
    "stem_words_validation_headline = []\n",
    "\n",
    "for i in range(len(train_bodies_stemmed)):\n",
    "    stem_words_train_bodies.append([stem(x) for x in clean_doc(train_bodies_stemmed.loc[i,'articleBody']).split()])\n",
    "for i in range(len(train_stances_stemmed)):\n",
    "    stem_words_train_headline.append([stem(x) for x in clean_doc(train_stances_stemmed.loc[i,'Headline']).split()])\n",
    "for i in range(len(validation_bodies_stemmed)):\n",
    "    stem_words_validation_bodies.append([stem(x) for x in clean_doc(validation_bodies_stemmed.loc[i,'articleBody']).split()])\n",
    "for i in range(len(validation_stances_stemmed)):\n",
    "    stem_words_validation_headline.append([stem(x) for x in clean_doc(validation_stances_stemmed.loc[i,'Headline']).split()])\n",
    "\n",
    "train_bodies_stemmed['articleBody words'] = stem_words_train_bodies\n",
    "train_stances_stemmed['Headline words'] = stem_words_train_headline\n",
    "validation_bodies_stemmed['articleBody words'] = stem_words_validation_bodies\n",
    "validation_stances_stemmed['Headline words'] = stem_words_validation_headline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove stop words (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_stances_filtered = train_stances_stemmed.copy()\n",
    "train_bodies_filtered = train_bodies_stemmed.copy()\n",
    "validation_stances_filtered = validation_stances_stemmed.copy()\n",
    "validation_bodies_filtered = validation_bodies_stemmed.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "train_stances_filtered['Headline words'] = train_stances_filtered['Headline words'].apply(lambda x: [w for w in x if not w in stop_words])\n",
    "train_bodies_filtered['articleBody words'] = train_bodies_filtered['articleBody words'].apply(lambda x: [w for w in x if not w in stop_words])\n",
    "validation_stances_filtered['Headline words'] = validation_stances_filtered['Headline words'].apply(lambda x: [w for w in x if not w in stop_words])\n",
    "validation_bodies_filtered['articleBody words'] = validation_bodies_filtered['articleBody words'].apply(lambda x: [w for w in x if not w in stop_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word occurrences count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract a master list of all words. Initiate a list to hold all words\n",
    "all_words = set([])\n",
    "# Loop through all Body and Headline texts and append new words to the master list\n",
    "for i in range(len(train_stances)):\n",
    "    all_words.update(train_stances.loc[i,'Headline words'])\n",
    "for i in range(len(train_bodies)):\n",
    "    all_words.update(train_bodies.loc[i,'articleBody words'])\n",
    "for i in range(len(validation_stances)):\n",
    "    all_words.update(validation_stances.loc[i,'Headline words'])\n",
    "for i in range(len(validation_bodies)):\n",
    "    all_words.update(validation_bodies.loc[i,'articleBody words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do the same but on stemmed words\n",
    "all_words_stemmed = set([])\n",
    "# Loop through all Body and Headline texts and append new words to the master list\n",
    "for i in range(len(train_stances_stemmed)):\n",
    "    all_words_stemmed.update(train_stances_stemmed.loc[i,'Headline words'])\n",
    "for i in range(len(train_bodies_stemmed)):\n",
    "    all_words_stemmed.update(train_bodies_stemmed.loc[i,'articleBody words'])\n",
    "for i in range(len(validation_stances_stemmed)):\n",
    "    all_words_stemmed.update(validation_stances_stemmed.loc[i,'Headline words'])\n",
    "for i in range(len(validation_bodies_stemmed)):\n",
    "    all_words_stemmed.update(validation_bodies_stemmed.loc[i,'articleBody words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do the same but on stemmed words with stop words filtered out\n",
    "all_words_filtered = set([])\n",
    "# Loop through all Body and Headline texts and append new words to the master list\n",
    "for i in range(len(train_stances_stemmed)):\n",
    "    all_words_filtered.update(train_stances_filtered.loc[i,'Headline words'])\n",
    "for i in range(len(train_bodies_stemmed)):\n",
    "    all_words_filtered.update(train_bodies_filtered.loc[i,'articleBody words'])\n",
    "for i in range(len(validation_stances_stemmed)):\n",
    "    all_words_filtered.update(validation_stances_filtered.loc[i,'Headline words'])\n",
    "for i in range(len(validation_bodies_stemmed)):\n",
    "    all_words_filtered.update(validation_bodies_filtered.loc[i,'articleBody words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define function to convert documents in dataset into vectors of word occurences\n",
    "def word_counter(df, words_vector):\n",
    "    import time\n",
    "    start = time.time()\n",
    "    \n",
    "    # Initiate the master array to hold vector representations for each document\n",
    "    master_vector = []\n",
    "    for i in range(len(df)):\n",
    "        # Initiate an empty array for each document\n",
    "        doc_vector = []\n",
    "        counter = Counter(df.loc[i])\n",
    "        for word in words_vector:\n",
    "            # Check number of times word in master list appears in document\n",
    "            doc_vector.append(counter[word])\n",
    "        master_vector.append(doc_vector)\n",
    "    \n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "    \n",
    "    return master_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.174811601638794\n",
      "3.442002773284912\n",
      "776.4593210220337\n",
      "94.89768719673157\n"
     ]
    }
   ],
   "source": [
    "# Compute word counts in each headlines & article bodies - for training and validation dataset\n",
    "train_bodies_vector = []\n",
    "train_bodies_vector = word_counter(train_bodies['articleBody words'], all_words)\n",
    "validation_bodies_vector = []\n",
    "validation_bodies_vector = word_counter(validation_bodies['articleBody words'], all_words)\n",
    "train_stances_vector = []\n",
    "train_stances_vector = word_counter(train_stances['Headline words'], all_words)\n",
    "validation_stances_vector = []\n",
    "validation_stances_vector = word_counter(validation_stances['Headline words'], all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.685055494308472\n",
      "1.1514663696289062\n",
      "280.6340591907501\n",
      "30.7632737159729\n"
     ]
    }
   ],
   "source": [
    "# Compute word counts (stemming discarded) in each headlines & article bodies - for training and validation dataset\n",
    "train_bodies_vector_stemmed = []\n",
    "train_bodies_vector_stemmed = word_counter(train_bodies_stemmed['articleBody words'], all_words_stemmed)\n",
    "validation_bodies_vector_stemmed = []\n",
    "validation_bodies_vector_stemmed = word_counter(validation_bodies_stemmed['articleBody words'], all_words_stemmed)\n",
    "train_stances_vector_stemmed = []\n",
    "train_stances_vector_stemmed = word_counter(train_stances_stemmed['Headline words'], all_words_stemmed)\n",
    "validation_stances_vector_stemmed = []\n",
    "validation_stances_vector_stemmed = word_counter(validation_stances_stemmed['Headline words'], all_words_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.730015754699707\n",
      "0.9939184188842773\n",
      "262.15616488456726\n",
      "27.414483070373535\n"
     ]
    }
   ],
   "source": [
    "# Compute word counts (stop words discarded) in each headlines & article bodies - for training and validation dataset\n",
    "train_bodies_vector_filtered = []\n",
    "train_bodies_vector_filtered = word_counter(train_bodies_filtered['articleBody words'], all_words_filtered)\n",
    "validation_bodies_vector_filtered = []\n",
    "validation_bodies_vector_filtered = word_counter(validation_bodies_filtered['articleBody words'], all_words_filtered)\n",
    "train_stances_vector_filtered = []\n",
    "train_stances_vector_filtered = word_counter(train_stances_filtered['Headline words'], all_words_filtered)\n",
    "validation_stances_vector_filtered = []\n",
    "validation_stances_vector_filtered = word_counter(validation_stances_filtered['Headline words'], all_words_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering, Modelling and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### Training dataset feature engineering #####\n",
      "Time elapsed for cosine similarity calculation: 265.7892382144928\n",
      "Time elapsed for KL Divergence: 6.93135666847229\n",
      "Time elapsed for KL Divergence with Dirichlet Smoothing: 44.43669843673706\n",
      "###### Validation dataset feature engineering #####\n",
      "Time elapsed for KL Divergence: 0.6870007514953613\n",
      "Time elapsed for KL Divergence with Dirichlet Smoothing: 4.4889702796936035\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "import feature_engineering\n",
    "train_stemmed, validation_stemmed = feature_engineering.feature_engineering(train_bodies_stemmed, train_stances_stemmed, train_bodies_vector_stemmed, train_stances_vector_stemmed, validation_bodies_stemmed, validation_stances_stemmed, validation_bodies_vector_stemmed, validation_stances_vector_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Linear Regression #####\n",
      "Confusion matrix: \n",
      "[[  74  154   93   71]\n",
      " [  10   31   19   29]\n",
      " [  92  227  228  171]\n",
      " [  43  194   80 3406]]\n",
      "Precision recall F1: \n",
      "(array([ 0.33789954,  0.05115512,  0.54285714,  0.92629861]), array([ 0.18877551,  0.34831461,  0.31754875,  0.91485361]), array([ 0.24222586,  0.08920863,  0.40070299,  0.92054054]), array([ 392,   89,  718, 3723], dtype=int64))\n",
      "##### Logistic Regression #####\n",
      "Confusion matrix: \n",
      "[[  63  166   96   67]\n",
      " [  10   31   19   29]\n",
      " [  89  237  231  161]\n",
      " [  40  228   84 3371]]\n",
      "Precision recall F1 score: \n",
      "(array([ 0.31188119,  0.04682779,  0.5372093 ,  0.92916207]), array([ 0.16071429,  0.34831461,  0.32172702,  0.90545259]), array([ 0.21212121,  0.08255659,  0.40243902,  0.91715413]), array([ 392,   89,  718, 3723], dtype=int64))\n",
      "##### Random Forest #####\n",
      "Confusion matrix: \n",
      "[[  63  166   96   67]\n",
      " [  10   31   19   29]\n",
      " [  89  237  231  161]\n",
      " [  40  228   84 3371]]\n",
      "Precision recall F1 score: \n",
      "(array([ 0.38904899,  0.58823529,  0.51630435,  0.95107202]), array([ 0.34438776,  0.11235955,  0.66155989,  0.92935804]), array([ 0.36535859,  0.18867925,  0.57997558,  0.94008966]), array([ 392,   89,  718, 3723], dtype=int64))\n",
      "Feature Importance: \n",
      "                                  Features  Importance\n",
      "3                       Jaccard similarity    0.210697\n",
      "0                        Cosine similarity    0.193959\n",
      "1          KL-divergence with no smoothing    0.183747\n",
      "2   KL-divergence with Dirichlet smoothing    0.125452\n",
      "20                           discuss: said    0.042898\n",
      "18                           discuss: told    0.027689\n",
      "11                       discuss: reported    0.025913\n",
      "13                          discuss: claim    0.018768\n",
      "4                       discuss: according    0.015305\n",
      "21                      discuss: statement    0.013876\n",
      "6                         discuss: appears    0.013574\n",
      "7                        discuss: believed    0.010884\n",
      "12                     discuss: reportedly    0.010719\n",
      "14                         discuss: claims    0.010483\n",
      "15                        discuss: claimed    0.010168\n",
      "24                      discuss: spokesman    0.009931\n",
      "27                           disagree: can    0.008528\n",
      "32                            disagree: no    0.008305\n",
      "19                      discuss: allegedly    0.008258\n",
      "5                      discuss: apparently    0.007719\n",
      "9                   discuss: investigation    0.007517\n",
      "22                         discuss: stated    0.005070\n",
      "35                           disagree: not    0.004211\n",
      "23                         discuss: states    0.004069\n",
      "17                          discuss: imply    0.003832\n",
      "8                   discuss: investigating    0.003798\n",
      "10                        discuss: ongoing    0.003614\n",
      "31                      disagree: reported    0.002744\n",
      "25                        discuss: unknown    0.002368\n",
      "28                         disagree: could    0.001721\n",
      "26                           disagree: may    0.001210\n",
      "29                         disagree: doubt    0.001188\n",
      "33                       disagree: experts    0.000727\n",
      "34                         disagree: still    0.000635\n",
      "30                        disagree: doubts    0.000272\n",
      "16                        discuss: implies    0.000150\n"
     ]
    }
   ],
   "source": [
    "# Modelling: Linear Regression, Logistic Regression & Random Forest\n",
    "import modelling\n",
    "modelling.modelling(train_stemmed, validation_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming discarded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### Training dataset feature engineering #####\n",
      "Time elapsed for cosine similarity calculation: 385.39739990234375\n",
      "Time elapsed for KL Divergence: 6.87669825553894\n",
      "Time elapsed for KL Divergence with Dirichlet Smoothing: 43.66496229171753\n",
      "###### Validation dataset feature engineering #####\n",
      "Time elapsed for KL Divergence: 0.6919715404510498\n",
      "Time elapsed for KL Divergence with Dirichlet Smoothing: 4.375027179718018\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "import feature_engineering\n",
    "train, validation = feature_engineering.feature_engineering(train_bodies, train_stances, train_bodies_vector, train_stances_vector, validation_bodies, validation_stances, validation_bodies_vector, validation_stances_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Linear Regression #####\n",
      "Confusion matrix: \n",
      "[[  58   57  212   65]\n",
      " [   5   12   47   25]\n",
      " [  58   72  433  155]\n",
      " [   3   41   81 3598]]\n",
      "Precision recall F1: \n",
      "(array([ 0.46774194,  0.06593407,  0.56015524,  0.93624772]), array([ 0.14795918,  0.13483146,  0.60306407,  0.96642493]), array([ 0.2248062 ,  0.08856089,  0.58081824,  0.95109701]), array([ 392,   89,  718, 3723], dtype=int64))\n",
      "##### Logistic Regression #####\n",
      "Confusion matrix: \n",
      "[[  26  138  151   77]\n",
      " [   1   28   33   27]\n",
      " [  27  168  356  167]\n",
      " [   0   61   36 3626]]\n",
      "Precision recall F1 score: \n",
      "(array([ 0.48148148,  0.07088608,  0.61805556,  0.93045933]), array([ 0.06632653,  0.31460674,  0.49582173,  0.97394574]), array([ 0.11659193,  0.11570248,  0.55023184,  0.95170604]), array([ 392,   89,  718, 3723], dtype=int64))\n",
      "##### Random Forest #####\n",
      "Confusion matrix: \n",
      "[[  26  138  151   77]\n",
      " [   1   28   33   27]\n",
      " [  27  168  356  167]\n",
      " [   0   61   36 3626]]\n",
      "Precision recall F1 score: \n",
      "(array([ 0.38123167,  0.3       ,  0.50103093,  0.96296296]), array([ 0.33163265,  0.06741573,  0.67688022,  0.92882084]), array([ 0.35470668,  0.11009174,  0.57582938,  0.94558381]), array([ 392,   89,  718, 3723], dtype=int64))\n",
      "Feature Importance: \n",
      "                                  Features  Importance\n",
      "1          KL-divergence with no smoothing    0.237684\n",
      "2   KL-divergence with Dirichlet smoothing    0.210254\n",
      "0                        Cosine similarity    0.170792\n",
      "3                       Jaccard similarity    0.132445\n",
      "20                           discuss: said    0.036168\n",
      "18                           discuss: told    0.025187\n",
      "11                       discuss: reported    0.022831\n",
      "13                          discuss: claim    0.017446\n",
      "4                       discuss: according    0.014145\n",
      "21                      discuss: statement    0.011972\n",
      "6                         discuss: appears    0.010920\n",
      "7                        discuss: believed    0.010228\n",
      "12                     discuss: reportedly    0.008968\n",
      "14                         discuss: claims    0.008819\n",
      "15                        discuss: claimed    0.008799\n",
      "27                           disagree: can    0.008417\n",
      "24                      discuss: spokesman    0.008350\n",
      "32                            disagree: no    0.007145\n",
      "5                      discuss: apparently    0.006751\n",
      "19                      discuss: allegedly    0.006655\n",
      "9                   discuss: investigation    0.005789\n",
      "22                         discuss: stated    0.004546\n",
      "35                           disagree: not    0.004305\n",
      "23                         discuss: states    0.003611\n",
      "17                          discuss: imply    0.003425\n",
      "10                        discuss: ongoing    0.003297\n",
      "8                   discuss: investigating    0.002399\n",
      "31                      disagree: reported    0.002150\n",
      "25                        discuss: unknown    0.002101\n",
      "28                         disagree: could    0.001434\n",
      "29                         disagree: doubt    0.001152\n",
      "26                           disagree: may    0.000485\n",
      "33                       disagree: experts    0.000446\n",
      "34                         disagree: still    0.000408\n",
      "30                        disagree: doubts    0.000262\n",
      "16                        discuss: implies    0.000216\n"
     ]
    }
   ],
   "source": [
    "# Modelling: Linear Regression, Logistic Regression & Random Forest\n",
    "import modelling\n",
    "modelling.modelling(train, validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and stop words discarded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### Training dataset feature engineering #####\n",
      "Time elapsed for cosine similarity calculation: 275.06541991233826\n",
      "Time elapsed for KL Divergence: 6.799327373504639\n",
      "Time elapsed for KL Divergence with Dirichlet Smoothing: 35.84059405326843\n",
      "###### Validation dataset feature engineering #####\n",
      "Time elapsed for KL Divergence: 0.6805007457733154\n",
      "Time elapsed for KL Divergence with Dirichlet Smoothing: 3.660275936126709\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "import feature_engineering\n",
    "train, validation = feature_engineering.feature_engineering(train_bodies_filtered, train_stances_filtered, train_bodies_vector_filtered, train_stances_vector_filtered, validation_bodies_filtered, validation_stances_filtered, validation_bodies_vector_filtered, validation_stances_vector_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Linear Regression #####\n",
      "Confusion matrix: \n",
      "[[ 143  137   30   82]\n",
      " [  29   31    5   24]\n",
      " [ 240  211  123  144]\n",
      " [   6   21    0 3696]]\n",
      "Precision recall F1: \n",
      "(array([ 0.34210526,  0.0775    ,  0.77848101,  0.9366447 ]), array([ 0.36479592,  0.34831461,  0.17130919,  0.99274778]), array([ 0.35308642,  0.12678937,  0.28082192,  0.96388056]), array([ 392,   89,  718, 3723], dtype=int64))\n",
      "##### Logistic Regression #####\n",
      "Confusion matrix: \n",
      "[[  96  117   88   91]\n",
      " [  18   25   19   27]\n",
      " [ 157  152  250  159]\n",
      " [   2   19    2 3700]]\n",
      "Precision recall F1 score: \n",
      "(array([ 0.35164835,  0.0798722 ,  0.69637883,  0.93034951]), array([ 0.24489796,  0.28089888,  0.34818942,  0.99382219]), array([ 0.2887218 ,  0.12437811,  0.46425255,  0.96103896]), array([ 392,   89,  718, 3723], dtype=int64))\n",
      "##### Random Forest #####\n",
      "Confusion matrix: \n",
      "[[  96  117   88   91]\n",
      " [  18   25   19   27]\n",
      " [ 157  152  250  159]\n",
      " [   2   19    2 3700]]\n",
      "Precision recall F1 score: \n",
      "(array([ 0.46496815,  0.76190476,  0.62984724,  0.97591006]), array([ 0.37244898,  0.17977528,  0.74651811,  0.97931775]), array([ 0.41359773,  0.29090909,  0.68323773,  0.97761094]), array([ 392,   89,  718, 3723], dtype=int64))\n",
      "Feature Importance: \n",
      "                                  Features  Importance\n",
      "0                        Cosine similarity    0.278585\n",
      "3                       Jaccard similarity    0.261359\n",
      "1          KL-divergence with no smoothing    0.172283\n",
      "2   KL-divergence with Dirichlet smoothing    0.077052\n",
      "20                           discuss: said    0.030635\n",
      "18                           discuss: told    0.021342\n",
      "11                       discuss: reported    0.020268\n",
      "13                          discuss: claim    0.014449\n",
      "4                       discuss: according    0.012091\n",
      "6                         discuss: appears    0.009631\n",
      "21                      discuss: statement    0.009550\n",
      "14                         discuss: claims    0.007941\n",
      "7                        discuss: believed    0.007738\n",
      "15                        discuss: claimed    0.007365\n",
      "24                      discuss: spokesman    0.007342\n",
      "12                     discuss: reportedly    0.006794\n",
      "27                           disagree: can    0.006526\n",
      "32                            disagree: no    0.006506\n",
      "5                      discuss: apparently    0.005931\n",
      "19                      discuss: allegedly    0.005917\n",
      "22                         discuss: stated    0.004469\n",
      "9                   discuss: investigation    0.004339\n",
      "35                           disagree: not    0.003467\n",
      "17                          discuss: imply    0.003153\n",
      "23                         discuss: states    0.002684\n",
      "8                   discuss: investigating    0.002554\n",
      "10                        discuss: ongoing    0.002494\n",
      "25                        discuss: unknown    0.002051\n",
      "31                      disagree: reported    0.001755\n",
      "29                         disagree: doubt    0.000951\n",
      "28                         disagree: could    0.000931\n",
      "34                         disagree: still    0.000580\n",
      "26                           disagree: may    0.000544\n",
      "33                       disagree: experts    0.000349\n",
      "30                        disagree: doubts    0.000267\n",
      "16                        discuss: implies    0.000109\n"
     ]
    }
   ],
   "source": [
    "# Modelling: Linear Regression, Logistic Regression & Random Forest\n",
    "import modelling\n",
    "modelling.modelling(train, validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
